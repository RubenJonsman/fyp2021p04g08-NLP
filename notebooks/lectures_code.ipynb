{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0359bcb2cd7216d5a07175250aa043472cdc6ad1624cf4dc94e99942968f9418f",
   "display_name": "Python 3.8.8 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lecture 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "line = 'A cat sat on the mat. His name was MÃ¥ns.'\n",
    "\n",
    "# Initialise lists\n",
    "tokens = []\n",
    "unmatchable = []\n",
    "\n",
    "# Compile patterns for speedup\n",
    "token_pat = re.compile(r'\\w+')\n",
    "skippable_pat = re.compile(r'\\s+')  # typically spaces\n",
    "\n",
    "# As long as there's any material left...\n",
    "while line:\n",
    "    # Try finding a skippable token delimiter first.\n",
    "    skippable_match = re.search(skippable_pat, line)\n",
    "    if skippable_match and skippable_match.start() == 0:\n",
    "        # If there is one at the beginning of the line, just skip it.\n",
    "        line = line[skippable_match.end():]\n",
    "    else:\n",
    "        # Else try finding a real token.\n",
    "        token_match = re.search(token_pat, line)\n",
    "        if token_match and token_match.start() == 0:\n",
    "            # If there is one at the beginning of the line, tokenise it.\n",
    "            tokens.append(line[:token_match.end()])\n",
    "            line = line[token_match.end():]\n",
    "        else:\n",
    "            # Else there is unmatchable material here.\n",
    "            # It ends where a skippable or token match starts, or at the end of the line.\n",
    "            unmatchable_end = len(line)\n",
    "            if skippable_match:\n",
    "                unmatchable_end = skippable_match.start()\n",
    "            if token_match:\n",
    "                unmatchable_end = min(unmatchable_end, token_match.start())\n",
    "            # Add it to unmatchable and discard from line.\n",
    "            unmatchable.append(line[:unmatchable_end])\n",
    "            line = line[unmatchable_end:]\n",
    "\n",
    "print(tokens)\n",
    "print(unmatchable)"
   ]
  },
  {
   "source": [
    "# Lecture 2\n",
    "\n",
    "Exercise: N-gram Language Modelling\n",
    "First Year Projects\n",
    "4 May 2021\n",
    "\n",
    "\n",
    "In this exercise, you will get familiar with n-gram language models using the\n",
    "nltk library.\n",
    "\n",
    "You can work with the corpora from the two tasks you selected for your project.\n",
    "Additionally, you can download a dataset of news editorials collected and\n",
    "distributed by the organisers of the Conference on Machine Translation from\n",
    "here:\n",
    "http://data.statmt.org/news-commentary/v16/training-monolingual/news-commentary-v16.en.gz\n",
    "\n",
    "This is a sizeable data set (albeit still quite small by the standards of the\n",
    "NLP community), and if you find that your computer becomes intolerably slow or\n",
    "runs into memory problems, you can just use a subset of the corpus for the\n",
    "exercises.\n",
    "\n",
    "1. Remove a subset of about 5000 sentences from the news commentary dataset to\n",
    "be used for evaluation. For the TweetEval datasets, you can use the standard\n",
    "training/validation split.\n",
    "\n",
    "2. Load and tokenise your datasets so that, for each of the corpora, you get a\n",
    "list of sentences, each sentence represented by a list of tokens. Use the same\n",
    "tokeniser for all datasets.\n",
    "\n",
    "3. Follow the instructions at\n",
    "http://www.nltk.org/api/nltk.lm.html#module-nltk.lm\n",
    "to train maximum-likelihood language models of varying orders (e.g., n=2..6) for\n",
    "each of your corpora. Make a note of the size of the n-gram lists for each\n",
    "n-gram order. You might also plot them.\n",
    "\n",
    "IMPORTANT: We will want to compare perplexities across different corpora. This\n",
    "only produces meaningful results if all the models use exactly the same\n",
    "vocabulary. To ensure that, create a vocabulary from the largest of your\n",
    "datasets and use it for ALL the corpora.\n",
    "\n",
    "4. Use the lm.generate function to generate some example text from each of your\n",
    "models and compare.\n",
    "\n",
    "5. Use lm.score and lm.logscore to calculate the scores of a couple of n-grams\n",
    "you find in the texts with the different models. Try 1 or 2 n-grams with all\n",
    "function words and 1 or 2 n-grams that contain specific content words or names.\n",
    "\n",
    "5. Use the lm.perplexity function to compute the perplexity of the validation\n",
    "sets from each dataset with each of the language models and compare. Keep in\n",
    "mind that LOWER perplexity is BETTER.\n",
    "\n",
    "6. Repeat the steps above with another language model implementation, such as\n",
    "Laplace, WittenBellInterpolated or KneserNeyInterpolated."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import nltk.tokenize\n",
    "import numpy\n",
    "import pandas\n",
    "import pickle\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# tok = nltk.tokenize.TreebankWordTokenizer()\n",
    "#\n",
    "# corpus = []\n",
    "# with open('news-commentary-v16.en', 'r') as f:\n",
    "#     for line in f:\n",
    "#         corpus.extend(t for line in f for t in tok.tokenize(line))\n",
    "#\n",
    "# with open('ncv16-list.pkl', 'wb') as f:\n",
    "#     pickle.dump(corpus, f)\n",
    "\n",
    "with open('ncv16-list.pkl', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "\n",
    "voc = collections.Counter(corpus)\n",
    "frq = pandas.DataFrame(voc.most_common(), columns=['token', 'frequency'])\n",
    "\n",
    "# Index in the sorted list\n",
    "frq['idx'] = frq.index + 1\n",
    "\n",
    "# Frequency normalised by corpus size\n",
    "frq['norm_freq'] = frq.frequency / len(corpus)\n",
    "\n",
    "# Cumulative normalised frequency\n",
    "frq['cumul_frq'] = frq.norm_freq.cumsum()\n",
    "\n",
    "seaborn.set_theme(style='whitegrid')\n",
    "\n",
    "# Plot: Cumulative frequency by index\n",
    "seaborn.relplot(x='idx', y='cumul_frq', data=frq)\n",
    "plt.show()\n",
    "\n",
    "# Plot: Cumulative frequency by index, top 10000 tokens\n",
    "seaborn.relplot(x='idx', y='cumul_frq', data=frq[:10000], kind='line')\n",
    "plt.show()\n",
    "\n",
    "# Plot: Log-log plot for Zipf's law\n",
    "frq['log_frq'] = numpy.log(frq.frequency)\n",
    "frq['log_rank'] = numpy.log(frq.frequency.rank(ascending=True))\n",
    "seaborn.relplot(x='log_rank', y='log_frq', data=frq)\n",
    "plt.show()\n",
    "\n",
    "pass"
   ]
  },
  {
   "source": [
    "# Lecture 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "inroot = 'tweeteval/datasets'\n",
    "outroot = 'iaa-sets'\n",
    "\n",
    "corpora = [\n",
    "    'emoji',\n",
    "    'emotion',\n",
    "    'hate',\n",
    "    'irony',\n",
    "    'offensive',\n",
    "    'sentiment',\n",
    "    'stance/abortion',\n",
    "    'stance/atheism',\n",
    "    'stance/climate',\n",
    "    'stance/feminist',\n",
    "    'stance/hillary'\n",
    "]\n",
    "\n",
    "iaa_size = 120\n",
    "\n",
    "for crp in corpora:\n",
    "    indir = inroot + '/' + crp\n",
    "    outdir = outroot + '/' + crp\n",
    "    with open(indir + '/train_text.txt', 'r') as f:\n",
    "        train_text = [line.rstrip('\\n') for line in f]\n",
    "    with open(indir + '/train_labels.txt', 'r') as f:\n",
    "        train_labels = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    train_size = len(train_text)\n",
    "    assert len(train_labels) == train_size\n",
    "\n",
    "    smpl = set(random.sample(range(train_size), iaa_size))\n",
    "    iaa_text = [t for i, t in enumerate(train_text) if i in smpl]\n",
    "    iaa_labels = [t for i, t in enumerate(train_labels) if i in smpl]\n",
    "\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    with open(outdir + '/iaa_text.txt', 'w') as f:\n",
    "        print('\\n'.join(iaa_text), file=f)\n",
    "    with open(outdir + '/iaa_labels.txt', 'w') as f:\n",
    "        print('\\n'.join(iaa_labels), file=f)\n",
    "    with open(outdir + '/iaa_indices.txt', 'w') as f:\n",
    "        print('\\n'.join(str(i) for i in sorted(smpl)), file=f)"
   ]
  }
 ]
}