{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0359bcb2cd7216d5a07175250aa043472cdc6ad1624cf4dc94e99942968f9418f",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Imports and functions loaded.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "%run -i ../functions.py"
   ]
  },
  {
   "source": [
    "# Task 1\n",
    "Build a tokanizer that works like the nltk `TweetTokenizer`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Regex library\n",
    "To make things a bit easier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported from: https://www.nlpnotebook.com/python/tokenization/nlp/normalization/project/2020/01/06/lets-make-a-tokenizer-part-3.html \n",
    "ALPHA = '[A-Z]+'\n",
    "DIGITS = '[0-9]'\n",
    "BOS = '^'\n",
    "EOS = '$'\n",
    "PLUS = '+'\n",
    "STAR = \"*\"\n",
    "PERIOD = r'\\.'\n",
    "INITIAL_PUNCTUATION = '[\\'\"]'\n",
    "FINAL_PUNCTUATION = '[\\',!?\":.]'\n",
    "CURRENCY_SYMBOL = '[$£¥€]'\n",
    "QUESTION_MARK = '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined by me\n",
    "NOTALPHA = r'[^a-zA-Z\\s]' # also excludes spaces\n",
    "HASHTAGS = r'#\\w+'\n",
    "WORD = r'\\w+'"
   ]
  },
  {
   "source": [
    "## Open file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/sentiment/train_text.txt', 'r') as f:\n",
    "    tweets = [line.strip() for line in f]"
   ]
  },
  {
   "source": [
    "## Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hashtags(string):\n",
    "    return re.findall(HASHTAGS, string)\n",
    "\n",
    "def find_nonwords(string):\n",
    "    return re.findall(NOTALPHA, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the simplest tokenizer possible - split only by space\n",
    "def split_space(string):\n",
    "    return string.split(' ')"
   ]
  },
  {
   "source": [
    "## Tokenizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_split = []\n",
    "for tweet in tweets:\n",
    "    tweet_split.append(split_space(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_only_tokenizer(listofstrings):\n",
    "    tokens = []\n",
    "    unmatchables = []\n",
    "\n",
    "    for string in listofstrings:\n",
    "        hashtags = re.findall(r'#(\\w+)', string)\n",
    "        tokens.append(hashtags)\n",
    "\n",
    "        not_hashtags = []\n",
    "        all_words = re.findall(r'\\w+', string)\n",
    "        for word in all_words:\n",
    "            if word not in hashtags:\n",
    "                not_hashtags.append(word)\n",
    "        unmatchables.append(not_hashtags)\n",
    "\n",
    "    return (tokens, unmatchables)\n",
    "\n",
    "    # /^((?!#(\\w+)).)*$/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['HappyBirthdayRemusLupin']\n['QT', 'user', 'In', 'the', 'original', 'draft', 'of', 'the', '7th', 'book', 'Remus', 'Lupin', 'survived', 'the', 'Battle', 'of', 'Hogwarts']\n\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"\n"
     ]
    }
   ],
   "source": [
    "test_hashtag = hashtag_only_tokenizer(tweets)\n",
    "print(test_hashtag[0][0])\n",
    "print(test_hashtag[1][0])\n",
    "print(tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonword_only_tokenizer(listofstrings):\n",
    "    tokens = []\n",
    "    unmatchables = []\n",
    "\n",
    "    for string in listofstrings:\n",
    "        tokens.append(re.findall(r'[^a-zA-Z\\s]', string))\n",
    "        unmatchables.append(re.findall(r'[\\w]+', string))\n",
    "\n",
    "    return (tokens, unmatchables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['\"', '@', '7', ',', '.', '#', '\"']\n['QT', 'user', 'In', 'the', 'original', 'draft', 'of', 'the', '7th', 'book', 'Remus', 'Lupin', 'survived', 'the', 'Battle', 'of', 'Hogwarts', 'HappyBirthdayRemusLupin']\n"
     ]
    }
   ],
   "source": [
    "test_nonword = nonword_only_tokenizer(tweets)\n",
    "print(test_nonword[0][0])\n",
    "print(test_nonword[1][0])"
   ]
  },
  {
   "source": [
    "### Testing functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['#HappyBirthdayRemusLupin']\n"
     ]
    }
   ],
   "source": [
    "# find all hashtags\n",
    "print(find_hashtags(tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['\"', '@', '7', ',', '.', '#', '\"']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "find_nonwords(tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['\"',\n",
       " 'QT',\n",
       " '@user',\n",
       " 'In',\n",
       " 'the',\n",
       " 'original',\n",
       " 'draft',\n",
       " 'of',\n",
       " 'the',\n",
       " '7th',\n",
       " 'book',\n",
       " ',',\n",
       " 'Remus',\n",
       " 'Lupin',\n",
       " 'survived',\n",
       " 'the',\n",
       " 'Battle',\n",
       " 'of',\n",
       " 'Hogwarts',\n",
       " '.',\n",
       " '#HappyBirthdayRemusLupin',\n",
       " '\"']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# baseline tokenizer\n",
    "TweetTokenizer().tokenize(tweets[0])"
   ]
  },
  {
   "source": [
    "## Working with dataframes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_train = pd.read_csv(\"../datasets/sentiment/train_text.txt\", sep=\"\\t\", names=[\"tweets\"], quoting=3)\n",
    "\n",
    "sentiment_test = pd.read_csv(\"../datasets/sentiment/test_text.txt\", sep=\"\\t\", names=[\"tweets\"], quoting=3)\n",
    "\n",
    "sentiment_val = pd.read_csv(\"../datasets/sentiment/val_text.txt\", sep=\"\\t\", names=[\"tweets\"], quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0        \"\n",
       "1        \"\n",
       "2        .\n",
       "3        '\n",
       "4        @\n",
       "        ..\n",
       "45610    @\n",
       "45611    9\n",
       "45612    1\n",
       "45613    @\n",
       "45614    (\n",
       "Name: tweets, Length: 45615, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/python-pandas-series-str-extractall/\n",
    "# https://stackoverflow.com/questions/42379389/finding-all-regex-matches-from-a-pandas-dataframe-column\n",
    "\n",
    "sentiment_train.tweets.str.extract(r'([^a-zA-Z\\s])', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "type(sentiment_train['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['\"', '@', '7', ',', '.', '#', '\"']"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# the fuction still works with dataframes, just feed in the column as series\n",
    "print(type(sentiment_train['tweets']))\n",
    "nonword_only_tokenizer(sentiment_train['tweets'])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasmus function\n",
    "def tokenize_ideal(line):\n",
    "    tokens = []\n",
    "    unmatchables = []\n",
    "    \n",
    "    for word in line.split():\n",
    "        if re.findall(r\"\\w+-\\w+|\\w+|[.&?%!#…]\", word) != []:\n",
    "            x = re.findall(r\"\\w+-\\w+|\\w+|[.&?%!#…]+\", word)\n",
    "            for element in x:\n",
    "                tokens.append(element)\n",
    "\n",
    "        if re.findall(r\"\\w+-\\w+|\\w+|[.&?%!#…]\", word) != [word] and re.findall(r\"[^\\w|.&!?%#…]+\", word) != []:\n",
    "            unmatchables.append(re.findall(r\"[^\\w|.!#?%…&]+\", word)[0])\n",
    "\n",
    "\n",
    "    return (tokens, unmatchables)"
   ]
  },
  {
   "source": [
    "## Difference between tokenizers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib # how to use this to compare 2 lists??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\" '"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "sentiment_train.tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = TweetTokenizer().tokenize(tweets[0])\n",
    "ideal_test = tokenize_ideal(sentiment_train.tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff = difflib.Differ().compare(baseline, ideal_test)\n",
    "# print('\\n'.join(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['\"',\n",
       " 'QT',\n",
       " '@user',\n",
       " 'In',\n",
       " 'the',\n",
       " 'original',\n",
       " 'draft',\n",
       " 'of',\n",
       " 'the',\n",
       " '7th',\n",
       " 'book',\n",
       " ',',\n",
       " 'Remus',\n",
       " 'Lupin',\n",
       " 'survived',\n",
       " 'the',\n",
       " 'Battle',\n",
       " 'of',\n",
       " 'Hogwarts',\n",
       " '.',\n",
       " '#HappyBirthdayRemusLupin',\n",
       " '\"']"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['QT',\n",
       " 'user',\n",
       " 'In',\n",
       " 'the',\n",
       " 'original',\n",
       " 'draft',\n",
       " 'of',\n",
       " 'the',\n",
       " '7th',\n",
       " 'book',\n",
       " 'Remus',\n",
       " 'Lupin',\n",
       " 'survived',\n",
       " 'the',\n",
       " 'Battle',\n",
       " 'of',\n",
       " 'Hogwarts',\n",
       " '.',\n",
       " '#',\n",
       " 'HappyBirthdayRemusLupin']"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "ideal_test[0]"
   ]
  },
  {
   "source": [
    "# Exercise 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = []\n",
    "\n",
    "for tweet in sentiment_train[\"tweets\"]:\n",
    "    tokenized_train.append(tokenize_ideal(tweet)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = []\n",
    "\n",
    "for tweet in sentiment_test[\"tweets\"]:\n",
    "    tokenized_test.append(tokenize_ideal(tweet)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_val = []\n",
    "\n",
    "for tweet in sentiment_val[\"tweets\"]:\n",
    "    tokenized_val.append(tokenize_ideal(tweet)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenized_train \n",
    "# test = tokenized_test"
   ]
  },
  {
   "source": [
    "The following was done as per: http://www.nltk.org/api/nltk.lm.html#module-nltk.lm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams # to train a bigram model\r\n",
    "from nltk.util import pad_sequence # pad sentences to know where each sentence beging and end, especially as they will be flattened later\r\n",
    "from nltk.lm.preprocessing import pad_both_ends # not sure how this differs from the above\r\n",
    "from nltk.util import everygrams\r\n",
    "from nltk.lm.preprocessing import flatten # instead of separating each sentence by having them in lists, the dataset is flattened with <s> and </s> indicating the start end end of sentences\r\n",
    "\r\n",
    "# pre processing\r\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline # generates train data and vocabulary automatically\r\n",
    "\r\n",
    "# train models\r\n",
    "from nltk.lm import MLE # train a Maximum Likelihood Estimator\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bigrams(text[0]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'QT',\n",
       " 'user',\n",
       " 'In',\n",
       " 'the',\n",
       " 'original',\n",
       " 'draft',\n",
       " 'of',\n",
       " 'the',\n",
       " '7th',\n",
       " 'book',\n",
       " 'Remus',\n",
       " 'Lupin',\n",
       " 'survived',\n",
       " 'the',\n",
       " 'Battle',\n",
       " 'of',\n",
       " 'Hogwarts',\n",
       " '.',\n",
       " '#',\n",
       " 'HappyBirthdayRemusLupin',\n",
       " '</s>']"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "list(pad_sequence(text[0], \n",
    "    pad_left=True,\n",
    "    left_pad_symbol=\"<s>\",\n",
    "    pad_right=True,\n",
    "    right_pad_symbol=\"</s>\",\n",
    "    n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'QT',\n",
       " 'user',\n",
       " 'In',\n",
       " 'the',\n",
       " 'original',\n",
       " 'draft',\n",
       " 'of',\n",
       " 'the',\n",
       " '7th',\n",
       " 'book',\n",
       " 'Remus',\n",
       " 'Lupin',\n",
       " 'survived',\n",
       " 'the',\n",
       " 'Battle',\n",
       " 'of',\n",
       " 'Hogwarts',\n",
       " '.',\n",
       " '#',\n",
       " 'HappyBirthdayRemusLupin',\n",
       " '</s>']"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "list(pad_both_ends(text[0], n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('<s>', 'QT'),\n",
       " ('QT', 'user'),\n",
       " ('user', 'In'),\n",
       " ('In', 'the'),\n",
       " ('the', 'original'),\n",
       " ('original', 'draft'),\n",
       " ('draft', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', '7th'),\n",
       " ('7th', 'book'),\n",
       " ('book', 'Remus'),\n",
       " ('Remus', 'Lupin'),\n",
       " ('Lupin', 'survived'),\n",
       " ('survived', 'the'),\n",
       " ('the', 'Battle'),\n",
       " ('Battle', 'of'),\n",
       " ('of', 'Hogwarts'),\n",
       " ('Hogwarts', '.'),\n",
       " ('.', '#'),\n",
       " ('#', 'HappyBirthdayRemusLupin'),\n",
       " ('HappyBirthdayRemusLupin', '</s>')]"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "list(bigrams(pad_both_ends(text[0], n=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('QT',),\n",
       " ('user',),\n",
       " ('In',),\n",
       " ('the',),\n",
       " ('original',),\n",
       " ('draft',),\n",
       " ('of',),\n",
       " ('the',),\n",
       " ('7th',),\n",
       " ('book',),\n",
       " ('Remus',),\n",
       " ('Lupin',),\n",
       " ('survived',),\n",
       " ('the',),\n",
       " ('Battle',),\n",
       " ('of',),\n",
       " ('Hogwarts',),\n",
       " ('.',),\n",
       " ('#',),\n",
       " ('HappyBirthdayRemusLupin',),\n",
       " ('</s>',),\n",
       " ('<s>', 'QT'),\n",
       " ('QT', 'user'),\n",
       " ('user', 'In'),\n",
       " ('In', 'the'),\n",
       " ('the', 'original'),\n",
       " ('original', 'draft'),\n",
       " ('draft', 'of'),\n",
       " ('of', 'the'),\n",
       " ('the', '7th'),\n",
       " ('7th', 'book'),\n",
       " ('book', 'Remus'),\n",
       " ('Remus', 'Lupin'),\n",
       " ('Lupin', 'survived'),\n",
       " ('survived', 'the'),\n",
       " ('the', 'Battle'),\n",
       " ('Battle', 'of'),\n",
       " ('of', 'Hogwarts'),\n",
       " ('Hogwarts', '.'),\n",
       " ('.', '#'),\n",
       " ('#', 'HappyBirthdayRemusLupin'),\n",
       " ('HappyBirthdayRemusLupin', '</s>')]"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "padded_bigrams = list(pad_both_ends(text[0], n=2))\n",
    "list(everygrams(padded_bigrams, max_len=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(flatten(pad_both_ends(sent, n=2) for sent in text));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In most cases we want to use the same text as the source for both vocabulary and ngram counts. Now that we understand what this means for our preprocessing, we can simply import a function that does everything for us.\n",
    "\n",
    "train, vocab = padded_everygram_pipeline(2, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "# Having prepared our data we are ready to start training a model. As a simple example, let us train a Maximum Likelihood Estimator (MLE). We only need to specify the highest ngram order to instantiate it.\n",
    "lm = MLE(2) \n",
    "# This automatically creates an empty vocabulary\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 57964 items>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "57964"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "# fit the model, whatever that means\n",
    "lm.fit(train, vocab)\n",
    "print(lm.vocab) \n",
    "#The vocabulary helps us handle words that have not occurred during training.\n",
    "len(lm.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<NgramCounter with 2 ngram orders and 2096573 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "lm.counts['Remus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "lm.counts[['#']]['Happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.03255269845597118"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "lm.score('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "lm.score(\"<UNK>\") == lm.score(\"jdfhasjdhaskj\")\n",
    "# True -> No occurences of \"jdfhasjdhaskj\" in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "# get the score for a word given some preceding context. For example we want to know what is the chance that “t” is preceded by “h”.\n",
    "lm.score(\"t\", [\"h\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-17.223298744892467\n-4.941079049219879\n"
     ]
    }
   ],
   "source": [
    "# to avoid underflow when working with small score values, use logscore method\n",
    "print(lm.logscore(\"Remus\"))\n",
    "print(lm.logscore(\"the\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm.score(test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['please', '.', '#', 'Broncos', 'Peyton']"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "lm.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5.609879131064775"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "test = list(bigrams(text[0]))\n",
    "lm.entropy(test) # wtf is this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "48.8362030201234"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "lm.perplexity(test) # wtf is this"
   ]
  },
  {
   "source": [
    "## Counting ngrams\n",
    "Whatever this is"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "text_bigrams = [ngrams(sent, 2) for sent in text]\n",
    "text_unigrams = [ngrams(sent, 1) for sent in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import NgramCounter\n",
    "ngram_counts = NgramCounter(text_bigrams + text_unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "34867"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "ngram_counts['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "ngram_counts['Remus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[48.8362030201234,\n",
       " 110.80707253157334,\n",
       " 84.28509337005167,\n",
       " 45.91584481487316,\n",
       " 109.0441935906818,\n",
       " 45.08854905757948]"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "mle_perplexity(sentiment_train['tweets'], 2)"
   ]
  },
  {
   "source": [
    "## Smoothing\n",
    "Need to figure out how to do Kneser-Ney smoothing.  \n",
    "Not sure what the input is for FreqDist and KneserNeyProbDist."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "alist = tweet_to_list('../datasets/irony/train_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\sabri\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('seeing', 'ppl', 'walking'),\n",
       " ('ppl', 'walking', 'w'),\n",
       " ('walking', 'w', 'crutches'),\n",
       " ('w', 'crutches', 'makes'),\n",
       " ('crutches', 'makes', 'me'),\n",
       " ('makes', 'me', 'really'),\n",
       " ('me', 'really', 'excited'),\n",
       " ('really', 'excited', 'for'),\n",
       " ('excited', 'for', 'the'),\n",
       " ('for', 'the', 'next'),\n",
       " ('the', 'next', '3'),\n",
       " ('next', '3', 'weeks'),\n",
       " ('3', 'weeks', 'of'),\n",
       " ('weeks', 'of', 'my'),\n",
       " ('of', 'my', 'life')]"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "token = tokenize_ideal(alist[0])\n",
    "token0 = list(ngrams(token[0], 3))\n",
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens_sent = [tokenize_ideal(item) for item in alist]\n",
    "trigrams = [ngrams(sent, 3) for sent in tokens_sent]\n",
    "fdist = FreqDist([item for l in trigrams for item in l])\n",
    "# fdists = [FreqDist(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "FreqDist({})"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "kneser_ney = nltk.KneserNeyProbDist(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "kneser_ney.prob(('I', 'can', 't'))"
   ]
  },
  {
   "source": [
    "# Exercise 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  nltk.metrics import agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "og = pd.read_csv(\"../datasets/annotations/iaa_labels.txt\", names=[\"og\"])\n",
    "sab = pd.read_csv(\"../datasets/annotations/annotations_sabrina.csv\", names=[\"sab\"])\n",
    "ida = pd.read_csv(\"../datasets/annotations/annotations_ida.csv\", names=[\"ida\"])\n",
    "rub = pd.read_csv(\"../datasets/annotations/annotations_ruben.csv\", names=[\"rub\"])\n",
    "ras = pd.read_csv(\"../datasets/annotations/annotations_rasmus.csv\", names=[\"ras\"])\n",
    "mag = pd.read_csv(\"../datasets/annotations/annotations_magnus.csv\", names=[\"mag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictonary\n",
    "us = [sab, ida, rub, ras, mag]\n",
    "og = [og, sab, ida, rub, ras, mag]\n",
    "\n",
    "us_df = pd.concat(us, axis = 1)\n",
    "og_df = pd.concat(og, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_list = ['sab', 'ida', 'rub', 'ras', 'mag']\n",
    "og_list = ['og', 'sab', 'ida', 'rub', 'ras', 'mag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_task(annotators_list, df):\n",
    "    ls = []\n",
    "    for annotator in annotators_list:\n",
    "        for i in range(len(df)):\n",
    "            ls.append((annotator, i, df[annotator][i]))\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_data = annotation_task(us_list, us_df)\n",
    "og_data = annotation_task(og_list, og_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average observed agreement: 0.6016666666666668\nScott's Pi: 0.19416471857579604\nCohen's Kappa: 0.2445540955840891\nFleiss's Kappa: 0.2251999135508971\nKrippendorf's Alpha: 0.19550777737816938\n"
     ]
    }
   ],
   "source": [
    "us_task = agreement.AnnotationTask(data=us_data)\n",
    "print(\"Average observed agreement:\", us_task.avg_Ao())\n",
    "print(\"Scott's Pi:\", us_task.pi())\n",
    "print(\"Cohen's Kappa:\", us_task.kappa())\n",
    "print(\"Fleiss's Kappa:\", us_task.multi_kappa())\n",
    "print(\"Krippendorf's Alpha:\", us_task.alpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average observed agreement: 0.6327777777777777\nScott's Pi: 0.25771459319509304\nCohen's Kappa: 0.29228296799333725\nFleiss's Kappa: 0.27701618783724646\nKrippendorf's Alpha: 0.2587455451489895\n"
     ]
    }
   ],
   "source": [
    "og_task = agreement.AnnotationTask(data=og_data)\n",
    "print(\"Average observed agreement:\", og_task.avg_Ao())\n",
    "print(\"Scott's Pi:\", og_task.pi())\n",
    "print(\"Cohen's Kappa:\", og_task.kappa())\n",
    "print(\"Fleiss's Kappa:\", og_task.multi_kappa())\n",
    "print(\"Krippendorf's Alpha:\", og_task.alpha())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<nltk.metrics.agreement.AnnotationTask at 0x28e27a82d60>"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "og_task"
   ]
  },
  {
   "source": [
    "# Exercise 4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_train = pd.read_csv(\"../datasets/irony/train_text.txt\", sep=\"\\t\", names=[\"tweets\"], quoting=3)\n",
    "irony_test = pd.read_csv(\"../datasets/irony/test_text.txt\", sep=\"\\t\", names=[\"tweets\"], quoting=3)\n",
    "irony_val = pd.read_csv(\"../datasets/irony/val_text.txt\", sep=\"\\t\", names=[\"tweets\"], quoting=3)\n",
    "\n",
    "irony_train_labels = pd.read_csv(\"../datasets/irony/train_labels.txt\", sep=\"\\t\", names=[\"tweets\"], quoting=3).values.flatten()\n",
    "irony_test_labels = pd.read_csv(\"../datasets/irony/test_labels.txt\", sep=\"\\t\", names=[\"tweets\"], quoting=3).values.flatten()\n",
    "irony_val_labels = pd.read_csv(\"../datasets/irony/val_labels.txt\", sep=\"\\t\", names=[\"tweets\"], quoting=3).values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2862"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "len(irony_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train_irony = []\n",
    "\n",
    "for tweet in irony_train[\"tweets\"]:\n",
    "    sentences_train_irony.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test_irony = []\n",
    "\n",
    "for tweet in irony_test[\"tweets\"]:\n",
    "    sentences_test_irony.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2862\n784\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences_train_irony))\n",
    "print(len(sentences_test_irony))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.mygreatlearning.com/blog/bag-of-words/\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "CountVec = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
    "                           stop_words='english')\n",
    "#transform\n",
    "Count_data = CountVec.fit_transform(sentences_train_irony)\n",
    " \n",
    "#create dataframe\n",
    "cv_dataframe = pd.DataFrame(Count_data.toarray(), columns = CountVec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      00  000  01273276865  034i  07  0_0  10  100  1000  1000sms  ...  \\\n",
       "0      0    0            0     0   0    0   0    0     0        0  ...   \n",
       "1      0    0            0     0   0    0   0    0     0        0  ...   \n",
       "2      0    0            0     0   0    0   0    0     0        0  ...   \n",
       "3      0    0            0     0   0    0   0    0     0        0  ...   \n",
       "4      0    0            0     0   0    0   0    0     0        0  ...   \n",
       "...   ..  ...          ...   ...  ..  ...  ..  ...   ...      ...  ...   \n",
       "2857   0    0            0     0   0    0   0    0     0        0  ...   \n",
       "2858   0    0            0     0   0    0   0    0     0        0  ...   \n",
       "2859   0    0            0     0   0    0   0    0     0        0  ...   \n",
       "2860   0    0            0     0   0    0   0    0     0        0  ...   \n",
       "2861   0    0            0     0   0    0   0    0     0        0  ...   \n",
       "\n",
       "      zebras  zen  zero  zimbabwe  zuckerberg  zzzz  ëœå  งวย  你就算5隔格我都知你讲me  \\\n",
       "0          0    0     0         0           0     0    0    0              0   \n",
       "1          0    0     0         0           0     0    0    0              0   \n",
       "2          0    0     0         0           0     0    0    0              0   \n",
       "3          0    0     0         0           0     0    0    0              0   \n",
       "4          0    0     0         0           0     0    0    0              0   \n",
       "...      ...  ...   ...       ...         ...   ...  ...  ...            ...   \n",
       "2857       0    0     0         0           0     0    0    0              0   \n",
       "2858       0    0     0         0           0     0    0    0              0   \n",
       "2859       0    0     0         0           0     0    0    0              0   \n",
       "2860       0    0     0         0           0     0    0    0              0   \n",
       "2861       0    0     0         0           0     0    0    0              0   \n",
       "\n",
       "      ｆｏｌｌｏｗ  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "...      ...  \n",
       "2857       0  \n",
       "2858       0  \n",
       "2859       0  \n",
       "2860       0  \n",
       "2861       0  \n",
       "\n",
       "[2862 rows x 7744 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>01273276865</th>\n      <th>034i</th>\n      <th>07</th>\n      <th>0_0</th>\n      <th>10</th>\n      <th>100</th>\n      <th>1000</th>\n      <th>1000sms</th>\n      <th>...</th>\n      <th>zebras</th>\n      <th>zen</th>\n      <th>zero</th>\n      <th>zimbabwe</th>\n      <th>zuckerberg</th>\n      <th>zzzz</th>\n      <th>ëœå</th>\n      <th>งวย</th>\n      <th>你就算5隔格我都知你讲me</th>\n      <th>ｆｏｌｌｏｗ</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2857</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2858</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2859</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2860</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2861</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2862 rows × 7744 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "cv_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.mygreatlearning.com/blog/bag-of-words/\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# without smooth IDF\n",
    "# define tf-idf\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
    "                        smooth_idf=False,  \n",
    "                        ngram_range=(1,1),stop_words='english') # to use only  bigrams ngram_range=(2,2)\n",
    "# transform\n",
    "tf_idf_data = tf_idf_vec.fit_transform(sentences_train_irony)\n",
    " \n",
    "# create dataframe\n",
    "tf_idf_dataframe = pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())\n",
    " \n",
    "# with smooth IDF\n",
    "tf_idf_vec_smooth = TfidfVectorizer(use_idf=True,  \n",
    "                        smooth_idf=True,  \n",
    "                        ngram_range=(1,1),stop_words='english')\n",
    " \n",
    " \n",
    "tf_idf_data_smooth = tf_idf_vec_smooth.fit_transform(sentences_train_irony)\n",
    "tf_idf_dataframe_smooth = pd.DataFrame(tf_idf_data_smooth.toarray(), columns = tf_idf_vec_smooth.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       00  000  01273276865  034i   07  0_0   10  100  1000  1000sms  ...  \\\n",
       "0     0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "1     0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "2     0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "3     0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "4     0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "...   ...  ...          ...   ...  ...  ...  ...  ...   ...      ...  ...   \n",
       "2857  0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "2858  0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "2859  0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "2860  0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "2861  0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "\n",
       "      zebras  zen  zero  zimbabwe  zuckerberg  zzzz  ëœå  งวย  你就算5隔格我都知你讲me  \\\n",
       "0        0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "1        0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "2        0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "3        0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "4        0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "...      ...  ...   ...       ...         ...   ...  ...  ...            ...   \n",
       "2857     0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "2858     0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "2859     0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "2860     0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "2861     0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "\n",
       "      ｆｏｌｌｏｗ  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "...      ...  \n",
       "2857     0.0  \n",
       "2858     0.0  \n",
       "2859     0.0  \n",
       "2860     0.0  \n",
       "2861     0.0  \n",
       "\n",
       "[2862 rows x 7744 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>01273276865</th>\n      <th>034i</th>\n      <th>07</th>\n      <th>0_0</th>\n      <th>10</th>\n      <th>100</th>\n      <th>1000</th>\n      <th>1000sms</th>\n      <th>...</th>\n      <th>zebras</th>\n      <th>zen</th>\n      <th>zero</th>\n      <th>zimbabwe</th>\n      <th>zuckerberg</th>\n      <th>zzzz</th>\n      <th>ëœå</th>\n      <th>งวย</th>\n      <th>你就算5隔格我都知你讲me</th>\n      <th>ｆｏｌｌｏｗ</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2857</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2858</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2859</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2860</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2861</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2862 rows × 7744 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "# without smooth IDF\n",
    "tf_idf_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       00  000  01273276865  034i   07  0_0   10  100  1000  1000sms  ...  \\\n",
       "0     0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "1     0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "2     0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "3     0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "4     0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "...   ...  ...          ...   ...  ...  ...  ...  ...   ...      ...  ...   \n",
       "2857  0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "2858  0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "2859  0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "2860  0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "2861  0.0  0.0          0.0   0.0  0.0  0.0  0.0  0.0   0.0      0.0  ...   \n",
       "\n",
       "      zebras  zen  zero  zimbabwe  zuckerberg  zzzz  ëœå  งวย  你就算5隔格我都知你讲me  \\\n",
       "0        0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "1        0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "2        0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "3        0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "4        0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "...      ...  ...   ...       ...         ...   ...  ...  ...            ...   \n",
       "2857     0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "2858     0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "2859     0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "2860     0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "2861     0.0  0.0   0.0       0.0         0.0   0.0  0.0  0.0            0.0   \n",
       "\n",
       "      ｆｏｌｌｏｗ  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "...      ...  \n",
       "2857     0.0  \n",
       "2858     0.0  \n",
       "2859     0.0  \n",
       "2860     0.0  \n",
       "2861     0.0  \n",
       "\n",
       "[2862 rows x 7744 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>00</th>\n      <th>000</th>\n      <th>01273276865</th>\n      <th>034i</th>\n      <th>07</th>\n      <th>0_0</th>\n      <th>10</th>\n      <th>100</th>\n      <th>1000</th>\n      <th>1000sms</th>\n      <th>...</th>\n      <th>zebras</th>\n      <th>zen</th>\n      <th>zero</th>\n      <th>zimbabwe</th>\n      <th>zuckerberg</th>\n      <th>zzzz</th>\n      <th>ëœå</th>\n      <th>งวย</th>\n      <th>你就算5隔格我都知你讲me</th>\n      <th>ｆｏｌｌｏｗ</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2857</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2858</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2859</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2860</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2861</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2862 rows × 7744 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "# with smooth IDF\n",
    "tf_idf_dataframe_smooth"
   ]
  },
  {
   "source": [
    "## Scikit Tutorial\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2862, 7991)"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(sentences_train_irony)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "count_vect.vocabulary_.get(u'apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2862, 7991)"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2862, 7991)"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "source": [
    "### MultinomialNB"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, irony_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_new_counts = count_vect.transform(sentences_test_irony)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "# create a dictonary\n",
    "data = {\"tweets\": pd.Series(sentences_test_irony),\n",
    "        \"actual\": pd.Series(irony_test_labels),\n",
    "        \"predicted\": pd.Series(predicted)}\n",
    "\n",
    "# dataframe of test data and predicted label\n",
    "df = pd.concat(data, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                tweets  actual  predicted\n",
       "0    @user Can U Help?||More conservatives needed o...       0          0\n",
       "1    Just walked in to #Starbucks and asked for a \"...       1          1\n",
       "2                                      #NOT GONNA WIN        0          0\n",
       "3    @user He is exactly that sort of person. Weirdo!        0          0\n",
       "4    So much #sarcasm at work mate 10/10 #boring 10...       1          1\n",
       "..                                                 ...     ...        ...\n",
       "779  If you drag yesterday into today, your tomorro...       0          0\n",
       "780  Congrats to my fav @user & her team & my birth...       0          1\n",
       "781  @user Jessica sheds tears at her fan signing e...       0          1\n",
       "782  #Irony: al jazeera is pro Anti - #GamerGate be...       1          1\n",
       "783  #NOT ALL 👌 There good & bad in every occupatio...       0          0\n",
       "\n",
       "[784 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n      <th>actual</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@user Can U Help?||More conservatives needed o...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Just walked in to #Starbucks and asked for a \"...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#NOT GONNA WIN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@user He is exactly that sort of person. Weirdo!</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>So much #sarcasm at work mate 10/10 #boring 10...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>779</th>\n      <td>If you drag yesterday into today, your tomorro...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>780</th>\n      <td>Congrats to my fav @user &amp; her team &amp; my birth...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>781</th>\n      <td>@user Jessica sheds tears at her fan signing e...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>782</th>\n      <td>#Irony: al jazeera is pro Anti - #GamerGate be...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>783</th>\n      <td>#NOT ALL 👌 There good &amp; bad in every occupatio...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>784 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6543367346938775"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "text_clf.fit(sentences_train_irony, irony_train_labels)\n",
    "\n",
    "predicted = text_clf.predict(sentences_test_irony)\n",
    "np.mean(predicted == irony_test_labels)"
   ]
  },
  {
   "source": [
    "### SGDClassifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6492346938775511"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='log', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "    ])\n",
    "\n",
    "text_clf.fit(sentences_train_irony, irony_train_labels)\n",
    "\n",
    "predicted = text_clf.predict(sentences_test_irony)\n",
    "np.mean(predicted == irony_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n  not ironic       0.73      0.66      0.69       473\n      ironic       0.55      0.63      0.59       311\n\n    accuracy                           0.65       784\n   macro avg       0.64      0.65      0.64       784\nweighted avg       0.66      0.65      0.65       784\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(irony_test_labels, predicted, target_names=['not ironic', 'ironic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[313, 160],\n",
       "       [115, 196]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "metrics.confusion_matrix(irony_test_labels, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'file_to_array' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sabri\\OneDrive\\Documents\\GitHub\\fyp2021p04g08\\functions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_hate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../datasets/hate/test_labels.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'file_to_array' is not defined"
     ]
    }
   ],
   "source": [
    "train_hate = file_to_array('../datasets/hate/test_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 352
    }
   ],
   "source": [
    "train_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "irony_train = pd.read_csv(\"../datasets/irony/train_text.txt\", sep=\"\\t\", quoting=3, names=[\"tweets\"])\n",
    "irony_test = pd.read_csv(\"../datasets/irony/test_text.txt\", sep=\"\\t\", quoting=3, names=[\"tweets\"])\n",
    "irony_val = pd.read_csv(\"../datasets/irony/val_text.txt\", sep=\"\\t\", quoting=3, names=[\"tweets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                tweets\n",
       "0    @user Can U Help?||More conservatives needed o...\n",
       "1    Just walked in to #Starbucks and asked for a \"...\n",
       "2                                      #NOT GONNA WIN \n",
       "3    @user He is exactly that sort of person. Weirdo! \n",
       "4    So much #sarcasm at work mate 10/10 #boring 10...\n",
       "..                                                 ...\n",
       "779  If you drag yesterday into today, your tomorro...\n",
       "780  Congrats to my fav @user & her team & my birth...\n",
       "781  @user Jessica sheds tears at her fan signing e...\n",
       "782  #Irony: al jazeera is pro Anti - #GamerGate be...\n",
       "783  #NOT ALL 👌 There good & bad in every occupatio...\n",
       "\n",
       "[784 rows x 1 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>@user Can U Help?||More conservatives needed o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Just walked in to #Starbucks and asked for a \"...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>#NOT GONNA WIN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@user He is exactly that sort of person. Weirdo!</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>So much #sarcasm at work mate 10/10 #boring 10...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>779</th>\n      <td>If you drag yesterday into today, your tomorro...</td>\n    </tr>\n    <tr>\n      <th>780</th>\n      <td>Congrats to my fav @user &amp; her team &amp; my birth...</td>\n    </tr>\n    <tr>\n      <th>781</th>\n      <td>@user Jessica sheds tears at her fan signing e...</td>\n    </tr>\n    <tr>\n      <th>782</th>\n      <td>#Irony: al jazeera is pro Anti - #GamerGate be...</td>\n    </tr>\n    <tr>\n      <th>783</th>\n      <td>#NOT ALL 👌 There good &amp; bad in every occupatio...</td>\n    </tr>\n  </tbody>\n</table>\n<p>784 rows × 1 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 354
    }
   ],
   "source": [
    "irony_test"
   ]
  },
  {
   "source": [
    "# Accuracy of prediction in diffent datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.7530211480362538, 0.7988372093023256)"
      ]
     },
     "metadata": {},
     "execution_count": 355
    }
   ],
   "source": [
    "SGD_accuracy('offensive', 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.6219895287958115, 0.6377551020408163)"
      ]
     },
     "metadata": {},
     "execution_count": 356
    }
   ],
   "source": [
    "SGD_accuracy('irony', 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.6212121212121212, 0.6428571428571429)"
      ]
     },
     "metadata": {},
     "execution_count": 357
    }
   ],
   "source": [
    "SGD_accuracy('stance/abortion', 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.7115384615384616, 0.65)"
      ]
     },
     "metadata": {},
     "execution_count": 358
    }
   ],
   "source": [
    "SGD_accuracy('stance/atheism', 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.725, 0.7100591715976331)"
      ]
     },
     "metadata": {},
     "execution_count": 359
    }
   ],
   "source": [
    "SGD_accuracy('stance/climate', 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.5970149253731343, 0.5473684210526316)"
      ]
     },
     "metadata": {},
     "execution_count": 360
    }
   ],
   "source": [
    "SGD_accuracy('stance/feminist', 'log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.6666666666666666, 0.6067796610169491)"
      ]
     },
     "metadata": {},
     "execution_count": 361
    }
   ],
   "source": [
    "SGD_accuracy('stance/hillary', 'log')"
   ]
  }
 ]
}